---
title: "Homework 4"
author: "[Taehwan Lee]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
format: html
# format: pdf
---

---

::: {.callout-important style="font-size: 0.8em;"}

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::


We will be using the following libraries:
```{R}
packages <- c(
  "dplyr", 
  "readr", 
  "tidyr", 
  "purrr", 
  "stringr", 
  "corrplot", 
  "car", 
  "caret", 
  "torch", 
  "nnet", 
  "broom"
)

# renv::install(packages)
sapply(packages, require, character.only=T)
```

<br><br><br><br>
---

## Question 1
::: {.callout-tip}
## 30 points
Automatic differentiation using `torch`
:::

###### 1.1 (5 points)

Consider $g(x, y)$ given by
$$
g(x, y) = (x - 3)^2 + (y - 4)^2.
$$

Using elementary calculus derive the expressions for

$$
\frac{d}{dx}g(x, y), \quad \text{and} \quad \frac{d}{dy}g(x, y).
$$

Using your answer from above, what is the answer to
$$
\frac{d}{dx}g(x, y) \Bigg|_{(x=3, y=4)} \quad \text{and} \quad \frac{d}{dy}g(x, y) \Bigg|_{(x=3, y=4)} ?
$$

Define $g(x, y)$ as a function in R, compute the gradient of $g(x, y)$ with respect to $x=3$ and $y=4$. Does the answer match what you expected? No I thought the number would be different.

```{R}
# Define the variables x and y, requires_grad=TRUE to enable gradient tracking
x <- torch_tensor(c(3), requires_grad = TRUE)
y <- torch_tensor(c(4), requires_grad = TRUE)

# Define the function g(x,y)
g <- (x - 3)^2 + (y - 4)^2

# Compute the gradients
g$backward()

# Access the gradients
x_grad <- x$grad
y_grad <- y$grad

x_grad
y_grad
```


---

###### 1.2 (10 points)


$$\newcommand{\u}{\boldsymbol{u}}\newcommand{\v}{\boldsymbol{v}}$$

Consider $h(\u, \v)$ given by
$$
h(\u, \v) = (\u \cdot \v)^3,
$$
where $\u \cdot \v$ denotes the dot product of two vectors, i.e., $\u \cdot \v = \sum_{i=1}^n u_i v_i.$

Using elementary calculus derive the expressions for the gradients

$$
\begin{aligned}
\nabla_\u h(\u, \v) &= \Bigg(\frac{d}{du_1}h(\u, \v), \frac{d}{du_2}h(\u, \v), \dots, \frac{d}{du_n}h(\u, \v)\Bigg)
\end{aligned}
$$

Using your answer from above, what is the answer to $\nabla_\u h(\u, \v)$ when $n=10$ and

$$
\begin{aligned}
\u = (-1, +1, -1, +1, -1, +1, -1, +1, -1, +1)\\
\v = (-1, -1, -1, -1, -1, +1, +1, +1, +1, +1)
\end{aligned}
$$

Define $h(\u, \v)$ as a function in R, initialize the two vectors $\u$ and $\v$ as `torch_tensor`s. Compute the gradient of $h(\u, \v)$ with respect to $\u$. Does the answer match what you expected? Yes

```{R}
# Define the vectors u and v
u <- torch_tensor(c(-1, 1, -1, 1, -1, 1, -1, 1, -1, 1), requires_grad=TRUE)
v <- torch_tensor(c(-1, -1, -1, -1, -1, 1, 1, 1, 1, 1))

# Define the function h(u, v) and compute its value
h <- (torch_dot(u, v) ^ 3)

# Compute the gradient with respect to u
h$backward()

# print the gradient of u
u$grad
```


---

###### 1.3 (5 points)

Consider the following function
$$
f(z) = z^4 - 6z^2 - 3z + 4
$$

Derive the expression for 
$$
f'(z_0) = \frac{df}{dz}\Bigg|_{z=z_0}
$$
and evaluate $f'(z_0)$ when $z_0 = -3.5$.

Define $f(z)$ as a function in R, and using the `torch` library compute $f'(-3.5)$. 

```{R}
# Define 
z <- torch_tensor(-3.5, requires_grad=TRUE)

# Define the function f(z)
f <- z^4 - 6*z^2 - 3*z + 4

# Compute the derivative with respect to z
f$backward()

# print the gradient of z
z$grad
```

---

###### 1.4 (5 points)

For the same function $f$, initialize $z[1] = -3.5$, and perform $n=100$ iterations of **gradient descent**, i.e., 

> $z[{k+1}] = z[k] - \eta f'(z[k]) \ \ \ \ $ for $k = 1, 2, \dots, 100$

Plot the curve $f$ and add taking $\eta = 0.02$, add the points $\{z_0, z_1, z_2, \dots z_{100}\}$ obtained using gradient descent to the plot. What do you observe?

```{R}
# Define the function f(z)
f <- function(z) {
  z^4 - 6 * z^2 - 3 * z + 4
}

# Define the derivative of f(z)
f_prime <- function(z) {
  4 * z^3 - 12 * z - 3
}

# Initialize parameters for gradient descent
z <- -3.5  # initial value of z
eta <- 0.02  # learning rate
n_iterations <- 100  # number of iterations

# Vector to store z values through iterations
z_values <- numeric(n_iterations + 1)
z_values[1] <- z  # Store initial value

# Perform gradient descent
for (k in 1:n_iterations) {
  z <- z - eta * f_prime(z)
  z_values[k + 1] <- z
}

# Sequence of z values for plotting
z_plot <- seq(-5, 3, length.out = 400)
f_plot <- sapply(z_plot, f)

# Plot f(z) using base R plot
plot(z_plot, f_plot, type = 'l', main = 'Function f(z) and Gradient Descent Points',
     xlab = 'z', ylab = 'f(z)', col = 'blue')
points(z_values, sapply(z_values, f), col = 'red', pch = 20)
```
The plot shows the function f(z) descending sharply from the left, with the red points representing the gradient descent path converging towards a local minimum near z=−2. These points indicate the iterative process of gradient descent, moving closer to the minimum with each step.

---

###### 1.5 (5 points)


Redo the same analysis as **Question 1.4**, but this time using $\eta = 0.03$. What do you observe? What can you conclude from this analysis

```{R}
# Update the learning rate
eta <- 0.03

# Reset the initial z value
z <- -3.5

# Initialize a vector to store z values
z_values <- numeric(n_iterations + 1)
z_values[1] <- z

# Perform gradient descent with the updated learning rate
for (k in 1:n_iterations) {
  z <- z - eta * f_prime(z)
  z_values[k + 1] <- z
}

# Plot f(z) using base R plot
plot(z_plot, f_plot, type = 'l', main = 'Function f(z) with Gradient Descent Points for eta = 0.03',
     xlab = 'z', ylab = 'f(z)', col = 'blue')
points(z_values, sapply(z_values, f), col = 'green', pch = 20)
```

The plot with a learning rate of 0.03 shows that the gradient descent points, represented by the green dots, quickly approach and overshoot the local minimum around z=−2 and instead converge to a different minimum around z=1. This behavior indicates that a larger learning rate can cause the gradient descent to miss the nearest minimum due to larger steps. This analysis suggests the important impact of the learning rate on the convergence and stability of the gradient descent algorithm.

<br><br><br><br>
<br><br><br><br>
---

## Question 2
::: {.callout-tip}
## 50 points
Logistic regression and interpretation of effect sizes
:::

For this question we will use the **Titanic** dataset from the Stanford data archive. This dataset contains information about passengers aboard the Titanic and whether or not they survived. 


---

###### 2.1 (5 points)

Read the data from the following URL as a tibble in R. Preprocess the data such that the variables are of the right data type, e.g., binary variables are encoded as factors, and convert all column names to lower case for consistency. Let's also rename the response variable `Survival` to `y` for convenience.

```{R}
url <- "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"

df <- read_csv(url)
```



---

###### 2.2 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using `corrplot()`

```{R}
df %>% select_if(is.numeric) %>% cor() %>% corrplot(method = "circle")
```




---

###### 2.3 (10 points)

Fit a logistic regression model to predict the probability of surviving the titanic as a function of:

* `pclass`
* `sex`
* `age`
* `fare`
* `# siblings`
* `# parents`

```{R}
# Rename columns to remove spaces and special characters
df <- df %>%
  rename(y = Survived,
         siblings_spouses = 'Siblings/Spouses Aboard',
         parents_children = 'Parents/Children Aboard') %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(c(Pclass, y), as.factor)) 

full_model <- glm(y ~ Pclass + Sex + Age + Fare + siblings_spouses + parents_children, data = df, family = "binomial")

summary(full_model)
```


---

###### 2.4 (30 points)

Provide an interpretation for the slope and intercept terms estimated in `full_model` in terms of the log-odds of survival in the titanic and in terms of the odds-ratio (if the covariate is also categorical).

::: {.callout-hint}
## 
Recall the definition of logistic regression from the lecture notes, and also recall how we interpreted the slope in the linear regression model (particularly when the covariate was categorical).
:::

The intercept in full_model represents the log-odds of survival for a passenger in the baseline category (1st class, female, with zero siblings/spouses and parents/children aboard, and with zero fare and age), estimated to be 4.109777. A positive coefficient, such as for the intercept, indicates higher log-odds of survival, which means a higher probability of survival. For categorical variables like Pclass and Sex, the coefficients (e.g., Pclass2, Pclass3, Sexmale) represent the change in log-odds of survival relative to the baseline category (1st class, female); for example, being male (Sexmale) decreases the log-odds of survival by 2.756710 compared to being female. In terms of odds ratios, exp(-2.756710) for Sexmale indicates that being male is associated with a decrease in the odds of survival by a factor of about 0.064 compared to being female, holding other variables constant.

<br><br><br><br>
<br><br><br><br>
---

## Question 3
::: {.callout-tip}
## 70 points

Variable selection and logistic regression in `torch`

:::


---

###### 3.1 (15 points)

Complete the following function `overview` which takes in two categorical vectors (`predicted` and `expected`) and outputs:

* The prediction accuracy
* The prediction error
* The false positive rate, and
* The false negative rate

```{R}
overview <- function(predicted, expected){
    accuracy <- mean(predicted == expected)
    error <- 1 - accuracy
    total_false_positives <- sum((predicted == 1) & (expected == 0))
    total_true_positives <- sum((predicted == 1) & (expected == 1))
    total_false_negatives <- sum((predicted == 0) & (expected == 1))
    total_true_negatives <- sum((predicted == 0) & (expected == 0))
    false_positive_rate <- total_false_positives / (total_false_positives + total_true_negatives)
    false_negative_rate <- total_false_negatives / (total_true_positives + total_false_negatives)
    return(
        data.frame(
            accuracy = accuracy, 
            error=error, 
            false_positive_rate = false_positive_rate, 
            false_negative_rate = false_negative_rate
        )
    )
}
```



You can check if your function is doing what it's supposed to do by evaluating
```{R}
overview(df$y, df$y)
```


and making sure that the accuracy is $100\%$ while the errors are $0\%$.
---

###### 3.2 (5 points)

Display an overview of the key performance metrics of `full_model`

```{R}
predicted_probabilities <- predict(full_model, df, type = "response")

# Convert probabilities to binary predictions based on a threshold (e.g., 0.5)
predicted_classes <- ifelse(predicted_probabilities > 0.5, 1, 0)

# Use the overview function to get the performance metrics
performance_metrics <- overview(predicted_classes, df$y)

performance_metrics
```


---

###### 3.3  (5 points)

Using backward-stepwise logistic regression, find a parsimonious altenative to `full_model`, and print its `overview`

```{R}
step_model <- step(full_model, direction = "backward")

summary(step_model)
```

```{R}
step_predicted <- predict(step_model, df, type = "response")

# Convert probabilities to binary predictions (using 0.5 as the threshold)
step_predicted_classes <- ifelse(step_predicted > 0.5, "1", "0")

# Use the overview function to get performance metrics of the simplified model
step_performance_metrics <- overview(step_predicted_classes, df$y)

step_performance_metrics
```

---

###### 3.4  (15 points)

Using the `caret` package, setup a **$5$-fold cross-validation** training method using the `caret::trainConrol()` function

```{R}
controls <- trainControl(method = "cv", number = 5, summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = "final")
```


Now, using `control`, perform $5$-fold cross validation using `caret::train()` to select the optimal $\lambda$ parameter for LASSO with logistic regression. 

Take the search grid for $\lambda$ to be in $\{ 2^{-20}, 2^{-19.5}, 2^{-19}, \dots, 2^{-0.5}, 2^{0} \}$.
```{R}
df$Sex <- as.factor(ifelse(df$Sex == "male", 0, 1))
df$y <- factor(df$y)
levels(df$y) <- make.names(levels(df$y))

```

```{R}

lasso_fit <- train(
  x = df %>% select(Pclass, Sex, Age, Fare, siblings_spouses, parents_children) %>% as.data.frame(),
  y = df$y,
  method = 'glmnet',
  trControl = controls, 
  tuneGrid = expand.grid(
    alpha = 1,
    lambda = 2^seq(-20, 0, by = 0.5)
    ),
  family = "binomial"
)
```


Using the information stored in `lasso_fit$results`, plot the results for  cross-validation accuracy vs. $log_2(\lambda)$. Choose the optimal $\lambda^*$, and report your results for this value of $\lambda^*$.
```{R}
ggplot(lasso_fit$results, aes(x = log2(lambda), y = ROC)) +  
  geom_line() +
  geom_point() +
  labs(title = "Cross-Validation ROC vs. log2(lambda)",
       x = "log2(lambda)", y = "Cross-Validation ROC") 

optimal_lambda <- lasso_fit$bestTune$lambda

# Report the optimal lambda
print(paste("Optimal lambda:", optimal_lambda))

# Extract and report the results for the optimal lambda
optimal_results <- subset(lasso_fit$results, lambda == optimal_lambda)
print(optimal_results)
```


---

###### 3.5  (25 points)

First, use the `model.matrix()` function to convert the covariates of `df` to a matrix format

```{R}
covariate_matrix <- model.matrix(full_model)[, -1]

```


Now, initialize the covariates $X$ and the response $y$ as `torch` tensors
```{R}
X <- torch_tensor(covariate_matrix[, -1], dtype = torch_float32())
y <- y_tensor <- torch_tensor(as.numeric(df$y), dtype = torch_float32())
```



Using the `torch` library, initialize an `nn_module` which performs logistic regression for this dataset. (Remember that we have 6 different covariates)
```{R}
logistic <- nn_module(
  initialize = function() {
    self$f <- nn_linear(in_features = 6, out_features = 1)
    self$g <- nn_sigmoid()
  },
  forward = function(x) {
    x %>% self$f() %>% self$g()
  }
)

f <- logistic()
```


You can verify that your code is right by checking that the output to the following code is a vector of probabilities:

```{R}
f(X)
```



Now, define the loss function `Loss()` which takes in two tensors `X` and `y` and a function `Fun`, and outputs the **Binary cross Entropy loss** between `Fun(X)` and `y`. 

```{R}
Loss <- function(X, y, Fun){
  pred <- Fun(X)
  
  # Compute Binary Cross Entropy Loss
  loss <- nnf_binary_cross_entropy(input = pred, target = y)
  
  return(loss$item())  
}
```



Initialize an optimizer using `optim_adam()` and perform $n=1000$ steps of gradient descent in order to fit logistic regression using `torch`.
```{R}
print(y$size())

```

```{R}
optimizer <- optim_adam(f$parameters, lr = 0.01)

n <- 1000

for (i in 1:n) {
  optimizer$zero_grad()
  
  # Compute predictions using the forward method of the model
  pred <- f$forward(X)
  
  # Compute loss using torch's binary cross entropy function
  # Ensure y is properly reshaped to match the predictions' shape
  loss <- nnf_binary_cross_entropy(input = pred, target = y$view(dim(pred)))
  
  # Backward pass to compute gradient
  loss$backward()
  
  # Update model parameters
  optimizer$step()
}
```

Using the final, optimized parameters of `f`, compute the compute the predicted results on `X`

```{R}
predicted_probabilities <- f(X) %>% as_array()
torch_predictions <- ifelse(predicted_probabilities > 0.5, 1, 0) #based on a threshold, 0.5

overview(torch_predictions, df$y)
```


---

###### 3.6  (5 points)

Create a summary table of the `overview()` summary statistics for each of the $4$ models we have looked at in this assignment, and comment on their relative strengths and drawbacks. 

The Full Model, which incorporated all available predictors, offered a baseline performance but likely suffered from over-fitting due to its complexity. The Stepwise Reduced Model, through backward elimination, presented a more parsimonious approach, improving interpretability without significantly compromising accuracy. The LASSO Model further refined this process by applying regularization, striking a balance between reducing overfitting and maintaining predictive power. Finally, the Torch Model, implemented via the torch library, showcased the potential of deep learning frameworks in optimizing model performance, possibly outperforming traditional logistic regression approaches in terms of accuracy and handling complex interactions between covariates. Each model has its merits, with the Stepwise Reduced and LASSO Models offering a good trade-off between simplicity and effectiveness, while the Torch Model stands out for its advanced optimization capabilities, albeit at the cost of increased computational demand and complexity in interpretation.


:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---



::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::